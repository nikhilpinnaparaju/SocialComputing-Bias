{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "import pandas\n",
    "from math import log\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.EMOJI,p.OPT.HASHTAG)\n",
    "    return p.tokenize(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24778</td>\n",
       "      <td>25291</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24779</td>\n",
       "      <td>25292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24780</td>\n",
       "      <td>25294</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24781</td>\n",
       "      <td>25295</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24782</td>\n",
       "      <td>25296</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0               0      3            0                   0        3      2   \n",
       "1               1      3            0                   3        0      1   \n",
       "2               2      3            0                   3        0      1   \n",
       "3               3      3            0                   2        1      1   \n",
       "4               4      6            0                   6        0      1   \n",
       "...           ...    ...          ...                 ...      ...    ...   \n",
       "24778       25291      3            0                   2        1      1   \n",
       "24779       25292      3            0                   1        2      2   \n",
       "24780       25294      3            0                   3        0      1   \n",
       "24781       25295      6            0                   6        0      1   \n",
       "24782       25296      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "...                                                  ...  \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24779  you've gone and broke the wrong heart baby, an...  \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781              youu got wild bitches tellin you lies  \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[24783 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pandas.read_csv('../HS_labeled_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Only Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI ,p.OPT.HASHTAG)\n",
    "    return p.tokenize(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(split_text):\n",
    "    sent2idx = []\n",
    "    for w in split_text:\n",
    "        if w.lower() in word2idx:\n",
    "            sent2idx.append(word2idx[w.lower()])\n",
    "        else:\n",
    "            sent2idx.append(word2idx['_UNK'])\n",
    "            \n",
    "    return sent2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22814</td>\n",
       "      <td>23293</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Who tf make an account of another bitch... Lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20932</td>\n",
       "      <td>21383</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>So I swore my school was closed, but that bitc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10127</td>\n",
       "      <td>10400</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't never be no bitch nigga , thats how yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>16016</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @IndiaMone: LOL RT @JoeBudden: Wait a minut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14495</td>\n",
       "      <td>14843</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @BrooksHeineman: @YoungPeezyy @512JORGE Wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6810</td>\n",
       "      <td>7001</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@p0rnoPuppy He called @waynebrady a nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21823</td>\n",
       "      <td>22289</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>These lil hoes sitting across da way too fucki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6697</td>\n",
       "      <td>6885</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@mis_sarahd @basedpapi1017 never said I wanted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18482</td>\n",
       "      <td>18891</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @_politeASSHOLE: 1) A real nigga would NEVE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16301</td>\n",
       "      <td>16673</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @LuchieCapone: &amp;#8220;Kik me&amp;#8221; ass hoe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18587 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "22814       23293      3            1                   2        0      1   \n",
       "20932       21383      3            0                   3        0      1   \n",
       "10127       10400      3            1                   2        0      1   \n",
       "15650       16016      3            0                   3        0      1   \n",
       "14495       14843      3            0                   3        0      1   \n",
       "...           ...    ...          ...                 ...      ...    ...   \n",
       "6810         7001      3            1                   2        0      1   \n",
       "21823       22289      3            0                   3        0      1   \n",
       "6697         6885      3            0                   3        0      1   \n",
       "18482       18891      3            1                   2        0      1   \n",
       "16301       16673      3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \n",
       "22814  Who tf make an account of another bitch... Lik...  \n",
       "20932  So I swore my school was closed, but that bitc...  \n",
       "10127  I can't never be no bitch nigga , thats how yo...  \n",
       "15650  RT @IndiaMone: LOL RT @JoeBudden: Wait a minut...  \n",
       "14495  RT @BrooksHeineman: @YoungPeezyy @512JORGE Wit...  \n",
       "...                                                  ...  \n",
       "6810          @p0rnoPuppy He called @waynebrady a nigger  \n",
       "21823  These lil hoes sitting across da way too fucki...  \n",
       "6697   @mis_sarahd @basedpapi1017 never said I wanted...  \n",
       "18482  RT @_politeASSHOLE: 1) A real nigga would NEVE...  \n",
       "16301  RT @LuchieCapone: &#8220;Kik me&#8221; ass hoe...  \n",
       "\n",
       "[18587 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356f467aeacd420cba5cbeb0cd8588df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18587.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train['clean_text'] = train.tweet.apply(lambda x: preprocess(x.lower().strip()))\n",
    "\n",
    "words = Counter()\n",
    "for sent in tqdm(train.clean_text.values):\n",
    "    words.update(w.lower() for w in sent)\n",
    "   \n",
    "# sort with most frequently occuring words first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# add <pad> and <unk> token to vocab which will be used later\n",
    "words = ['_PAD','_UNK'] + words\n",
    "\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}\n",
    "\n",
    "train['sentence2idx'] = train.clean_text.apply(lambda x: indexer(x))\n",
    "train['length'] = train.clean_text.apply(lambda x: len(x))\n",
    "train['label'] = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = plt.plot(train.length.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "valid['clean_text'] = valid.tweet.apply(lambda x: preprocess(x.strip()))\n",
    "\n",
    "valid['sentence2idx'] = valid.clean_text.apply(lambda x: indexer(x))\n",
    "valid['length'] = valid.clean_text.apply(lambda x: len(x))\n",
    "valid['label'] = valid['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df, maxlen=30):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "#         print('Padding')\n",
    "        self.df['padded_text'] = self.df.sentence2idx.apply(lambda x: self.pad_data(x))\n",
    "        self.padded_text = list(self.df.padded_text)\n",
    "        self.labels = list(self.df.label)\n",
    "        self.lengths = list(self.df.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         lens = self.df.length[idx]\n",
    "        X = self.padded_text[idx]\n",
    "        y = self.labels[idx]\n",
    "        lens = self.lengths[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_loader = VectorizeData(train)\n",
    "valid_loader = VectorizeData(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    }
   ],
   "source": [
    "tl = DataLoader(dataset=train_loader, batch_size=100, shuffle=True)\n",
    "print(len(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "vl = DataLoader(dataset=valid_loader, batch_size=100, shuffle=False)\n",
    "print(len(vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[tensor([[    2, 22358,    36,  ...,     0,     0,     0],\n",
      "        [   48,   205,    22,  ...,     0,     0,     0],\n",
      "        [    4,     6,   183,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,     5,  1015,  ...,     0,     0,     0],\n",
      "        [  425,    74,  2288,  ...,     0,     0,     0],\n",
      "        [    4,     2,   530,  ...,     0,     0,     0]]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1,\n",
      "        1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 0, 1]), tensor([ 3,  7, 11,  8, 13,  7, 12, 17, 13, 18,  6, 21, 13, 17, 13,  8,  7,  9,\n",
      "        18, 25, 10,  6, 17, 24, 24,  9,  3, 13, 11, 10, 16, 10, 11, 22, 14, 17,\n",
      "        21, 10, 11, 24,  9, 20, 18, 12, 22, 25, 21, 12, 10, 14, 10, 10, 11, 13,\n",
      "        11, 19, 27,  8,  8, 11,  9,  4,  7,  4, 18, 18,  6, 23, 18,  8, 13, 12,\n",
      "        18, 19, 20, 25, 10,  3, 27, 10, 17, 21, 26, 19, 16, 11, 17, 15, 22, 19,\n",
      "        12, 11, 25,  7,  5, 13, 21, 22, 13,  9])]\n"
     ]
    }
   ],
   "source": [
    "for i, samples in enumerate(tl):\n",
    "    print(i)\n",
    "    print(samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[tensor([[  308,    20,   211,  ...,     0,     0,     0],\n",
      "        [    4,     6,     2,  ...,     0,     0,     0],\n",
      "        [    1,  1384,  1399,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   526,  1269,  ...,     0,     0,     0],\n",
      "        [12010,  1709,   613,  ...,     0,     0,     0],\n",
      "        [    4,     6,  3712,  ...,     0,     0,     0]]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
      "        0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,\n",
      "        2, 2, 1, 1]), tensor([27, 10,  4,  8,  7, 27,  8, 13, 23, 17, 21, 22, 12, 16,  4,  9, 17, 24,\n",
      "        14, 15,  7,  5, 12, 26, 12, 28, 16,  7, 10, 26,  8, 14,  6, 17, 10,  7,\n",
      "        17,  7,  8, 14, 22, 21, 12,  9, 20, 12, 17,  7, 12, 18,  9,  6, 14, 19,\n",
      "         7, 22, 14, 24, 15, 16, 17,  7, 10, 26, 18,  6, 11,  8,  7,  3, 15,  9,\n",
      "        25,  8,  7, 30, 12, 25, 17, 14, 24, 13, 24, 10, 22,  6, 19, 23,  6, 10,\n",
      "         8, 12, 13, 17, 20, 11, 20,  3, 22, 17])]\n"
     ]
    }
   ],
   "source": [
    "for i, samples in enumerate(vl):\n",
    "    print(i)\n",
    "    print(samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = defaultdict(lambda:defaultdict(lambda:0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePMIMatrix(listOfTokenizedSentences):\n",
    "    wordCounts = defaultdict(lambda:0)\n",
    "    \n",
    "    print('Calculating Word Probabilities')\n",
    "    for tokenizedSent in tqdm(listOfTokenizedSentences):\n",
    "        for word in set(tokenizedSent):\n",
    "            wordCounts[word] += 1\n",
    "            \n",
    "    for key in wordCounts:\n",
    "        wordCounts[key] = wordCounts[key] / len(listOfTokenizedSentences)\n",
    "    \n",
    "    pairwiseCounts = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    \n",
    "    print('Calculating PairWise Probabilities')\n",
    "    for tokenizedSent in tqdm(listOfTokenizedSentences):\n",
    "        sentWords = set(tokenizedSent)\n",
    "        \n",
    "        for i in sentWords:\n",
    "            for j in sentWords:\n",
    "                pairwiseCounts[i][j] += 1 / len(listOfTokenizedSentences)\n",
    "        \n",
    "    return wordCounts, pairwiseCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Word Probabilities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc85950fe814dae8435d1120c50772f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18587.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating PairWise Probabilities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhil.pinnaparaju/anaconda3/envs/fakenews/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d037f73a326f4b29bcce1d5b22e41d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18587.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a,b = computePMIMatrix(list(train['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(w1,w2):\n",
    "    return max( 0,log(b[w1][w2]) - (log(a[w1])+log(a[w2])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return adj_normalized.A\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (\n",
    "        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    # t_k.append(sp.eye(adj.shape[0]))\n",
    "    # t_k.append(scaled_laplacian)\n",
    "    t_k.append(sp.eye(adj.shape[0]).A)\n",
    "    t_k.append(scaled_laplacian.A)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    # return sparse_to_tuple(t_k)\n",
    "    return t_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecArch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidir, rnnType,device):\n",
    "        super(RecArch, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.rnnType = rnnType\n",
    "        self.bidirectional = bidir\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.numDirs = 2\n",
    "        else:\n",
    "            self.numDirs = 1\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        if self.rnnType == 'lstm':\n",
    "            self.recNN = nn.LSTM(embedding_dim,hidden_dim, num_layers,batch_first=True,bidirectional=self.bidirectional)\n",
    "            \n",
    "        if self.rnnType == 'gru':\n",
    "            self.recNN = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True,bidirectional=self.bidirectional)\n",
    "            \n",
    "        if self.rnnType == 'rnn':\n",
    "            self.recNN = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, nonlinearity='tanh',bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(self.numDirs*hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embs = self.emb(x)\n",
    "        embs = embs.view(x.size(0),-1,self.embedding_dim).to(self.device)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "        \n",
    "        if self.rnnType == 'lstm':        \n",
    "            c0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "            \n",
    "            out,(hn,cn) = self.recNN(embs,(h0,c0))\n",
    "        \n",
    "        else:\n",
    "            out, hn = self.recNN(embs, h0)\n",
    "        \n",
    "#         print(out[:,-1,:].shape)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__( self, input_dim, output_dim, support, act_func = None, featureless = False, dropout_rate = 0., bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.support = support\n",
    "        self.featureless = featureless\n",
    "\n",
    "        for i in range(len(self.support)):\n",
    "            setattr(self, 'W{}'.format(i), nn.Parameter(torch.randn(input_dim, output_dim)))\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(1, output_dim))\n",
    "\n",
    "        self.act_func = act_func\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(len(self.support)):\n",
    "            if self.featureless:\n",
    "                pre_sup = getattr(self, 'W{}'.format(i))\n",
    "            else:\n",
    "                pre_sup = x.mm(getattr(self, 'W{}'.format(i)))\n",
    "            \n",
    "            if i == 0:\n",
    "                out = self.support[i].mm(pre_sup)\n",
    "            else:\n",
    "                out += self.support[i].mm(pre_sup)\n",
    "\n",
    "        if self.act_func is not None:\n",
    "            out = self.act_func(out)\n",
    "\n",
    "        self.embedding = out\n",
    "        return out\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, support, dropout_rate=0., num_classes=3):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.layer1 = GraphConvolution(input_dim, input_dim, support, act_func=nn.ReLU(), featureless=True, dropout_rate=dropout_rate)\n",
    "        self.layer2 = GraphConvolution(input_dim, num_classes, support, dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal combination seems to be with GRU of 50 units and 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "embedding_dim = 256\n",
    "n_hidden = 50\n",
    "n_out = 3\n",
    "num_layers = 1\n",
    "rnnType = 'gru'\n",
    "bidir = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecArch(\n",
       "  (emb): Embedding(27639, 256)\n",
       "  (recNN): GRU(256, 50, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecArch(vocab_size,embedding_dim,n_hidden,n_out,num_layers,bidir,rnnType,device)\n",
    "model = model.to(device)\n",
    "model.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN for Embedding Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:01<04:25,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8537766300839251 recall 0.5610869518451179 prec: 0.5220176337293658 f1: 0.5407825545036117\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       365\n",
      "           1       0.89      0.94      0.92      4822\n",
      "           2       0.67      0.74      0.71      1009\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6196\n",
      "   macro avg       0.52      0.56      0.54      6196\n",
      "weighted avg       0.80      0.85      0.83      6196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [00:55<02:47,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8316655907036798 recall 0.5581593417857038 prec: 0.6528652987640897 f1: 0.5896614412179718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.25      0.26       365\n",
      "           1       0.87      0.95      0.91      4822\n",
      "           2       0.82      0.47      0.60      1009\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      6196\n",
      "   macro avg       0.65      0.56      0.59      6196\n",
      "weighted avg       0.82      0.83      0.82      6196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [01:49<01:51,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8268237572627501 recall 0.550398815824912 prec: 0.6515347440905827 f1: 0.5824186157425463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.25      0.26       365\n",
      "           1       0.86      0.95      0.90      4822\n",
      "           2       0.82      0.45      0.58      1009\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      6196\n",
      "   macro avg       0.65      0.55      0.58      6196\n",
      "weighted avg       0.82      0.83      0.81      6196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 151/200 [02:43<00:55,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8271465461588121 recall 0.5695483482585696 prec: 0.6502678383790824 f1: 0.5958407102380332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.28      0.28       365\n",
      "           1       0.87      0.94      0.90      4822\n",
      "           2       0.81      0.48      0.61      1009\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      6196\n",
      "   macro avg       0.65      0.57      0.60      6196\n",
      "weighted avg       0.82      0.83      0.82      6196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [03:35<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=0.01)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "seq_dim = 30\n",
    "num_epochs = 200\n",
    "\n",
    "train_losses_iterwise = []\n",
    "recall_iterwise = []\n",
    "precision_iterwise = []\n",
    "accuracy_iterwise = []\n",
    "f1score_iterwise = []\n",
    "val_losses_iterwise = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i, (text,label,lengths) in enumerate(tl):\n",
    "\n",
    "        text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "        label = Variable(label).to(device)\n",
    "        \n",
    "#         print(sexism_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text)\n",
    "        \n",
    "#         print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "        train_losses.append(loss.data.cpu())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        allLabels = []\n",
    "        allPreds = []\n",
    "\n",
    "        for i, (text,label,lengths) in enumerate(vl):\n",
    "            labels=[]\n",
    "            text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "            label = Variable(label).to(device)\n",
    "\n",
    "            predicted = model(text)\n",
    "            predicted =  torch.softmax(predicted,1)\n",
    "            predicted = torch.max(predicted, 1)[1].cpu().numpy().tolist()\n",
    "    #                 print(predicted)\n",
    "    #                 print(sexism_label)\n",
    "            allLabels += (label.cpu().numpy().tolist())\n",
    "            allPreds += (predicted)\n",
    "\n",
    "        valacc = accuracy_score(allLabels, allPreds)\n",
    "        recscore = recall_score(allLabels, allPreds,average='macro')\n",
    "        precscore = precision_score(allLabels, allPreds,average='macro')\n",
    "        f1score = f1_score(allLabels, allPreds,average='macro')\n",
    "        cr = classification_report(allLabels, allPreds)\n",
    "        print(f'acc: {valacc} recall {recscore} prec: {precscore} f1: {f1score}')\n",
    "        print(cr)\n",
    "\n",
    "        train_losses_iterwise.append(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(vocab_size,embedding_dim,n_hidden,n_out,num_layers,rnnType,device):\n",
    "    model = RecArch(vocab_size,embedding_dim,n_hidden,n_out,num_layers,rnnType,device)\n",
    "    model.to(device)\n",
    "    model.float()\n",
    "    \n",
    "    optimizer = torch.optim.Adagrad(model.parameters(),lr=0.01)\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    count = 0\n",
    "    seq_dim = 30\n",
    "    num_epochs = 50\n",
    "\n",
    "    train_losses_iterwise = []\n",
    "    recall_iterwise = []\n",
    "    precision_iterwise = []\n",
    "    accuracy_iterwise = []\n",
    "    f1score_iterwise = []\n",
    "    val_losses_iterwise = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        for i, (text,label,lengths) in enumerate(tl):\n",
    "\n",
    "            text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "            label = Variable(label).to(device)\n",
    "\n",
    "    #         print(sexism_label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text)\n",
    "\n",
    "    #         print(outputs)\n",
    "\n",
    "            loss = criterion(outputs, label)\n",
    "            train_losses.append(loss.data.cpu())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "\n",
    "        if epoch % 50 == 0:    \n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            allLabels = []\n",
    "            allPreds = []\n",
    "\n",
    "            for i, (text,label,lengths) in enumerate(vl):\n",
    "                labels=[]\n",
    "                text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "                label = Variable(label).to(device)\n",
    "\n",
    "                predicted = model(text)\n",
    "                predicted =  torch.softmax(predicted,1)\n",
    "                predicted = torch.max(predicted, 1)[1].cpu().numpy().tolist()\n",
    "    #                 print(predicted)\n",
    "    #                 print(sexism_label)\n",
    "                allLabels += (label.cpu().numpy().tolist())\n",
    "                allPreds += (predicted)\n",
    "\n",
    "            valacc = accuracy_score(allLabels, allPreds)\n",
    "            recscore = recall_score(allLabels, allPreds,average='macro')\n",
    "            precscore = precision_score(allLabels, allPreds,average='macro')\n",
    "            f1score = f1_score(allLabels, allPreds,average='macro')\n",
    "            cr = classification_report(allLabels, allPreds)\n",
    "\n",
    "            accuracy_iterwise.append(valacc)\n",
    "            f1score_iterwise.append(f1score)\n",
    "    #                 print(f'acc: {valacc} recall {recscore} prec: {precscore} f1: {f1score}')\n",
    "    #                 print(cr)\n",
    "\n",
    "            train_losses_iterwise.append(np.mean(train_losses))\n",
    "                \n",
    "    return max(accuracy_iterwise),max(f1score_iterwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "vocab_size = len(words)\n",
    "embedding_dim = 256\n",
    "n_hidden = 100\n",
    "n_out = 4\n",
    "num_layers = 1\n",
    "rnnType = 'gru'\n",
    "\n",
    "n_hiddens = [5,25,50,100,200]\n",
    "n_layers = [1,2,3]\n",
    "rnnTypes = ['gru','lstm','rnn']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "results = {}\n",
    "    \n",
    "for n_hidden in n_hiddens:\n",
    "    for num_layers in n_layers:\n",
    "        for rnnType in rnnTypes:\n",
    "            key = (n_hidden,num_layers,rnnType)\n",
    "            acc,f1 = train_model(vocab_size,embedding_dim,n_hidden,n_out,num_layers,rnnType,device)\n",
    "            \n",
    "            results[key] = (acc,f1)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
