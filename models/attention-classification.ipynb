{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "import pandas\n",
    "from math import log\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.EMOJI,p.OPT.HASHTAG)\n",
    "    return p.tokenize(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('../HS_labeled_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI ,p.OPT.HASHTAG)\n",
    "    return p.tokenize(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(split_text):\n",
    "    sent2idx = []\n",
    "    for w in split_text:\n",
    "        if w.lower() in word2idx:\n",
    "            sent2idx.append(word2idx[w.lower()])\n",
    "        else:\n",
    "            sent2idx.append(word2idx['_UNK'])\n",
    "            \n",
    "    return sent2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = train.tweet.apply(lambda x: preprocess(x.lower().strip()))\n",
    "\n",
    "words = Counter()\n",
    "for sent in tqdm(train.clean_text.values):\n",
    "    words.update(w.lower() for w in sent)\n",
    "   \n",
    "# sort with most frequently occuring words first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# add <pad> and <unk> token to vocab which will be used later\n",
    "words = ['_PAD','_UNK'] + words\n",
    "\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}\n",
    "\n",
    "train['sentence2idx'] = train.clean_text.apply(lambda x: indexer(x))\n",
    "train['length'] = train.clean_text.apply(lambda x: len(x))\n",
    "train['label'] = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = plt.plot(train.length.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['clean_text'] = valid.tweet.apply(lambda x: preprocess(x.strip()))\n",
    "\n",
    "valid['sentence2idx'] = valid.clean_text.apply(lambda x: indexer(x))\n",
    "valid['length'] = valid.clean_text.apply(lambda x: len(x))\n",
    "valid['label'] = valid['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df, maxlen=30):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "#         print('Padding')\n",
    "        self.df['padded_text'] = self.df.sentence2idx.apply(lambda x: self.pad_data(x))\n",
    "        self.padded_text = list(self.df.padded_text)\n",
    "        self.labels = list(self.df.label)\n",
    "        self.lengths = list(self.df.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         lens = self.df.length[idx]\n",
    "        X = self.padded_text[idx]\n",
    "        y = self.labels[idx]\n",
    "        lens = self.lengths[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = VectorizeData(train)\n",
    "valid_loader = VectorizeData(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader(dataset=train_loader, batch_size=100, shuffle=True)\n",
    "print(len(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl = DataLoader(dataset=valid_loader, batch_size=100, shuffle=False)\n",
    "print(len(vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, samples in enumerate(tl):\n",
    "    print(i)\n",
    "    print(samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, samples in enumerate(vl):\n",
    "    print(i)\n",
    "    print(samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecAttnArch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidir, rnnType, attnType,device):\n",
    "        super(RecAttnArch, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.rnnType = rnnType\n",
    "        self.attnType = attnType\n",
    "        self.bidirectional = bidir\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.numDirs = 2\n",
    "        else:\n",
    "            self.numDirs = 1\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        if self.rnnType == 'lstm':\n",
    "            self.recNN = nn.LSTM(embedding_dim,hidden_dim, num_layers,batch_first=True,bidirectional=self.bidirectional)\n",
    "            \n",
    "        if self.rnnType == 'gru':\n",
    "            self.recNN = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True,bidirectional=self.bidirectional)\n",
    "            \n",
    "        if self.rnnType == 'rnn':\n",
    "            self.recNN = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, nonlinearity='tanh',bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.query_vector = nn.Parameter(torch.rand(hidden_dim*self.numDirs,1)).float()\n",
    "        \n",
    "        self.attnWgtMatrixSize = [self.numDirs*self.hidden_dim, self.numDirs*self.hidden_dim]\n",
    "        self.attnWgtMatrix = nn.Parameter(torch.randn(self.attnWgtMatrixSize).float()) # Multiplicative Attention\n",
    "    \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        if self.attnType == 'dot':\n",
    "            self.fc = nn.Linear(self.numDirs*self.hidden_dim, output_dim)\n",
    "        \n",
    "        if self.attnType == 'self':\n",
    "            self.fc = nn.Linear(self.numDirs*30*self.hidden_dim, output_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self,x,encMode=False):\n",
    "        embs = self.emb(x)\n",
    "        embs = embs.view(x.size(0),-1,self.embedding_dim).to(self.device)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "        \n",
    "        if self.rnnType == 'lstm':        \n",
    "            c0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "            \n",
    "            out,(hn,cn) = self.recNN(embs,(h0,c0))\n",
    "        \n",
    "        else:\n",
    "            out, hn = self.recNN(embs, h0)\n",
    "        \n",
    "        if self.attnType == 'dot':\n",
    "            Hw = out\n",
    "            attn_weights = self.softmax(Hw.matmul(self.query_vector))\n",
    "\n",
    "            out = out.mul(attn_weights)\n",
    "            context_vector = torch.sum(out,dim=1)\n",
    "            \n",
    "            fc_out = context_vector\n",
    "            \n",
    "        if self.attnType == 'self':\n",
    "            queryMatrix = out\n",
    "            keyMatrix = out.permute(0,2,1)\n",
    "            \n",
    "            attnScores = torch.bmm( torch.matmul(queryMatrix,self.attnWgtMatrix), keyMatrix )\n",
    "            attnScores = F.softmax(attnScores, dim=2)\n",
    "            hidden_matrix = torch.bmm(attnScores, queryMatrix)\n",
    "            \n",
    "            fc_out = hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2])\n",
    "        \n",
    "        if encMode:\n",
    "            return fc_out\n",
    "            \n",
    "        else:\n",
    "            return self.fc(fc_out)\n",
    "        \n",
    "    \n",
    "    def getScores(self,x):\n",
    "        embs = self.emb(x)\n",
    "        embs = embs.view(x.size(0),-1,self.embedding_dim).to(self.device)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "        \n",
    "        if self.rnnType == 'lstm':        \n",
    "            c0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "            \n",
    "            out,(hn,cn) = self.recNN(embs,(h0,c0))\n",
    "        \n",
    "        else:\n",
    "            out, hn = self.recNN(embs, h0)\n",
    "        \n",
    "        if self.attnType == 'dot':\n",
    "            Hw = out\n",
    "            attn_weights = self.softmax(Hw.matmul(self.query_vector))\n",
    "            out = out.mul(attn_weights)\n",
    "            \n",
    "            return out\n",
    "            \n",
    "        if self.attnType == 'self':\n",
    "            queryMatrix = out\n",
    "            keyMatrix = out.permute(0,2,1)\n",
    "            \n",
    "            attnScores = torch.bmm( torch.matmul(queryMatrix,self.attnWgtMatrix), keyMatrix )\n",
    "            attnScores = F.softmax(attnScores, dim=2)\n",
    "            \n",
    "            return attnScores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal combination seems to be with GRU of 50 units and 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "embedding_dim = 256\n",
    "n_hidden = 50\n",
    "n_out = 4\n",
    "num_layers = 1\n",
    "rnnType = 'gru'\n",
    "bidir = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:2'\n",
    "    \n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "model = model = RecAttnArch(vocab_size,embedding_dim,n_hidden,n_out,num_layers,bidir,rnnType,'self',device)\n",
    "model.to(device)\n",
    "model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(model.parameters())\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "seq_dim = 30\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses_iterwise = []\n",
    "recall_iterwise = []\n",
    "precision_iterwise = []\n",
    "accuracy_iterwise = []\n",
    "f1score_iterwise = []\n",
    "val_losses_iterwise = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i, (text,label,lengths) in enumerate(tl):\n",
    "\n",
    "        text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "        label = Variable(label).to(device)\n",
    "        \n",
    "#         print(sexism_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text)\n",
    "        \n",
    "#         print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "        train_losses.append(loss.data.cpu())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        allLabels = []\n",
    "        allPreds = []\n",
    "        probPreds = []\n",
    "\n",
    "        for i, (text,label,lengths) in enumerate(vl):\n",
    "            labels=[]\n",
    "            text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "            label = Variable(label).to(device)\n",
    "\n",
    "            predicted = model(text)\n",
    "            predicted =  torch.softmax(predicted,1)\n",
    "            probPreds.append(predicted)\n",
    "            predicted = torch.max(predicted, 1)[1].cpu().numpy().tolist()\n",
    "    #                 print(predicted)\n",
    "    #                 print(sexism_label)\n",
    "            allLabels += (label.cpu().numpy().tolist())\n",
    "            allPreds += (predicted)\n",
    "\n",
    "        valacc = accuracy_score(allLabels, allPreds)\n",
    "        recscore = recall_score(allLabels, allPreds,average='macro')\n",
    "        precscore = precision_score(allLabels, allPreds,average='macro')\n",
    "        f1score = f1_score(allLabels, allPreds,average='macro')\n",
    "#         roc = roc_auc_score(allLabels,allPreds)\n",
    "        cr = classification_report(allLabels, allPreds)\n",
    "        print(f'acc: {valacc} F1 {f1score}')\n",
    "        print(cr)\n",
    "\n",
    "        train_losses_iterwise.append(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    wgts = model.getScores(text[i].reshape(1,30,1)).to(device).detach().cpu().reshape(30,30)\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "    plt.matshow(wgts, cmap='hot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
