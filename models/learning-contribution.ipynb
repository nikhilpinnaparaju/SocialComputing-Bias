{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import preprocessor as p\n",
    "import numpy as np\n",
    "import pandas\n",
    "from math import log\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.EMOJI,p.OPT.HASHTAG)\n",
    "    return p.tokenize(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('../HS_labeled_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI ,p.OPT.HASHTAG)\n",
    "    return p.tokenize(text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer(split_text):\n",
    "    sent2idx = []\n",
    "    for w in split_text:\n",
    "        if w.lower() in word2idx:\n",
    "            sent2idx.append(word2idx[w.lower()])\n",
    "        else:\n",
    "            sent2idx.append(word2idx['_UNK'])\n",
    "            \n",
    "    return sent2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_text'] = train.tweet.apply(lambda x: preprocess(x.lower().strip()))\n",
    "\n",
    "words = Counter()\n",
    "for sent in tqdm(train.clean_text.values):\n",
    "    words.update(w.lower() for w in sent)\n",
    "   \n",
    "# sort with most frequently occuring words first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# add <pad> and <unk> token to vocab which will be used later\n",
    "words = ['_PAD','_UNK'] + words\n",
    "\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}\n",
    "\n",
    "train['sentence2idx'] = train.clean_text.apply(lambda x: indexer(x))\n",
    "train['length'] = train.clean_text.apply(lambda x: len(x))\n",
    "train['label'] = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['clean_text'] = valid.tweet.apply(lambda x: preprocess(x.strip()))\n",
    "\n",
    "valid['sentence2idx'] = valid.clean_text.apply(lambda x: indexer(x))\n",
    "valid['length'] = valid.clean_text.apply(lambda x: len(x))\n",
    "valid['label'] = valid['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df, maxlen=30):\n",
    "        self.maxlen = maxlen\n",
    "        self.df = df\n",
    "#         print('Padding')\n",
    "        self.df['padded_text'] = self.df.sentence2idx.apply(lambda x: self.pad_data(x))\n",
    "        self.padded_text = list(self.df.padded_text)\n",
    "        self.labels = list(self.df.label)\n",
    "        self.lengths = list(self.df.length)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         lens = self.df.length[idx]\n",
    "        X = self.padded_text[idx]\n",
    "        y = self.labels[idx]\n",
    "        lens = self.lengths[idx]\n",
    "        return X,y,lens\n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
    "        if len(s) > self.maxlen: padded[:] = s[:self.maxlen]\n",
    "        else: padded[:len(s)] = s\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = VectorizeData(train)\n",
    "valid_loader = VectorizeData(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader(dataset=train_loader, batch_size=100, shuffle=True)\n",
    "print(len(tl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl = DataLoader(dataset=valid_loader, batch_size=100, shuffle=False)\n",
    "print(len(vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, samples in enumerate(tl):\n",
    "    print(i)\n",
    "    print(samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, samples in enumerate(vl):\n",
    "    print(i)\n",
    "    print(samples)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePMIMatrix(listOfTokenizedSentences):\n",
    "    wordCounts = defaultdict(lambda:0)\n",
    "    \n",
    "    print('Calculating Word Probabilities')\n",
    "    for tokenizedSent in tqdm(listOfTokenizedSentences):\n",
    "        for word in set(tokenizedSent):\n",
    "            wordCounts[word] += 1\n",
    "            \n",
    "    for key in wordCounts:\n",
    "        wordCounts[key] = wordCounts[key] / len(listOfTokenizedSentences)\n",
    "    \n",
    "    pairwiseCounts = defaultdict(lambda:defaultdict(lambda:0))\n",
    "    \n",
    "    print('Calculating PairWise Probabilities')\n",
    "    for tokenizedSent in tqdm(listOfTokenizedSentences):\n",
    "        sentWords = set(tokenizedSent)\n",
    "        \n",
    "        for i in sentWords:\n",
    "            for j in sentWords:\n",
    "                pairwiseCounts[i][j] += 1 / len(listOfTokenizedSentences)\n",
    "        \n",
    "    return wordCounts, pairwiseCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = computePMIMatrix(list(train['clean_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(w1,w2):\n",
    "    try:\n",
    "        return max( 0,log(b[w1][w2]) - (log(a[w1])+log(a[w2])) )\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return adj_normalized.A\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (\n",
    "        2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    # t_k.append(sp.eye(adj.shape[0]))\n",
    "    # t_k.append(scaled_laplacian)\n",
    "    t_k.append(sp.eye(adj.shape[0]).A)\n",
    "    t_k.append(scaled_laplacian.A)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    # return sparse_to_tuple(t_k)\n",
    "    return t_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecArch(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidir, rnnType,device):\n",
    "        super(RecArch, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.rnnType = rnnType\n",
    "        self.bidirectional = bidir\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.numDirs = 2\n",
    "        else:\n",
    "            self.numDirs = 1\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        if self.rnnType == 'lstm':\n",
    "            self.recNN = nn.LSTM(embedding_dim,hidden_dim, num_layers,batch_first=True,bidirectional=self.bidirectional)\n",
    "            \n",
    "        if self.rnnType == 'gru':\n",
    "            self.recNN = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True,bidirectional=self.bidirectional)\n",
    "            \n",
    "        if self.rnnType == 'rnn':\n",
    "            self.recNN = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, nonlinearity='tanh',bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(self.numDirs*hidden_dim,output_dim)\n",
    "    \n",
    "    def encode(self,x):\n",
    "        embs = self.emb(x)\n",
    "        embs = embs.view(x.size(0),-1,self.embedding_dim).to(self.device)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "        \n",
    "        if self.rnnType == 'lstm':        \n",
    "            c0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "            \n",
    "            out,(hn,cn) = self.recNN(embs,(h0,c0))\n",
    "        \n",
    "        else:\n",
    "            out, hn = self.recNN(embs, h0)\n",
    "        \n",
    "#         print(out[:,-1,:].shape)\n",
    "        return out[:, -1, :]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        embs = self.emb(x)\n",
    "        embs = embs.view(x.size(0),-1,self.embedding_dim).to(self.device)\n",
    "        \n",
    "        h0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "        \n",
    "        if self.rnnType == 'lstm':        \n",
    "            c0 = Variable(torch.zeros(self.numDirs*self.num_layers,x.size(0),self.hidden_dim),requires_grad=True).to(self.device)\n",
    "            \n",
    "            out,(hn,cn) = self.recNN(embs,(h0,c0))\n",
    "        \n",
    "        else:\n",
    "            out, hn = self.recNN(embs, h0)\n",
    "        \n",
    "#         print(out[:,-1,:].shape)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal combination seems to be with GRU of 50 units and 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(words)\n",
    "embedding_dim = 256\n",
    "n_hidden = 50\n",
    "n_out = 3\n",
    "num_layers = 1\n",
    "rnnType = 'gru'\n",
    "bidir = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = RecArch(vocab_size,embedding_dim,n_hidden,n_out,num_layers,bidir,rnnType,device)\n",
    "model = model.to(device)\n",
    "model.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=0.01)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "seq_dim = 30\n",
    "num_epochs = 200\n",
    "\n",
    "train_losses_iterwise = []\n",
    "recall_iterwise = []\n",
    "precision_iterwise = []\n",
    "accuracy_iterwise = []\n",
    "f1score_iterwise = []\n",
    "val_losses_iterwise = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i, (text,label,lengths) in enumerate(tl):\n",
    "\n",
    "        text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "        label = Variable(label).to(device)\n",
    "        \n",
    "#         print(sexism_label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text)\n",
    "        \n",
    "#         print(outputs)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "        train_losses.append(loss.data.cpu())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        allLabels = []\n",
    "        allPreds = []\n",
    "        probPreds = []\n",
    "\n",
    "        for i, (text,label,lengths) in enumerate(vl):\n",
    "            labels=[]\n",
    "            text = Variable(text.view(-1, seq_dim, 1)).to(device)\n",
    "            label = Variable(label).to(device)\n",
    "\n",
    "            predicted = model(text)\n",
    "            predicted =  torch.softmax(predicted,1)\n",
    "            probPreds.append(predicted)\n",
    "            predicted = torch.max(predicted, 1)[1].cpu().numpy().tolist()\n",
    "    #                 print(predicted)\n",
    "    #                 print(sexism_label)\n",
    "            allLabels += (label.cpu().numpy().tolist())\n",
    "            allPreds += (predicted)\n",
    "\n",
    "        valacc = accuracy_score(allLabels, allPreds)\n",
    "        recscore = recall_score(allLabels, allPreds,average='macro')\n",
    "        precscore = precision_score(allLabels, allPreds,average='macro')\n",
    "        f1score = f1_score(allLabels, allPreds,average='macro')\n",
    "#         roc = roc_auc_score(allLabels,allPreds)\n",
    "        cr = classification_report(allLabels, allPreds)\n",
    "#         print(f'acc: {valacc} AUC {roc}')\n",
    "        print(cr)\n",
    "\n",
    "        train_losses_iterwise.append(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader(dataset=train_loader, batch_size=1, shuffle=True)\n",
    "vl = DataLoader(dataset=valid_loader, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAdjMatrix(text):\n",
    "    text = text.reshape(-1).tolist()\n",
    "    words = [idx2word[idx] for idx in text]\n",
    "    matrix = []\n",
    "    for i in range(len(words)):\n",
    "        row = []\n",
    "        for j in range(len(words)):\n",
    "            row.append(PPMI(words[i],words[j]))\n",
    "        row.append(1)\n",
    "        matrix.append(row)\n",
    "        \n",
    "    matrix.append([1 for i in range(len(words)+1)])\n",
    "    return preprocess_adj(np.array(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size,seq_dim):\n",
    "        super(GraphConvLayer,self).__init__()\n",
    "        \n",
    "        self.attn = nn.parameter.Parameter(torch.FloatTensor(seq_dim, seq_dim))\n",
    "        self.weight = nn.parameter.Parameter(torch.FloatTensor(in_size, out_size))\n",
    "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
    "        self.weight.data.normal_(0,var)\n",
    "        var = 2./(self.attn.size(1)+self.attn.size(0))\n",
    "        self.attn.data.normal_(0,var)\n",
    "        \n",
    "    def forward(self,X,A_hat):\n",
    "        X = torch.mm(X, self.weight)\n",
    "        wgtScores = torch.mm(A_hat, self.attn)\n",
    "        out = F.relu(torch.mm(wgtScores,X))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def getScores(self,A_hat):\n",
    "        wgtScores = torch.mm(A_hat, self.attn)\n",
    "        return wgtScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvNet(nn.Module):\n",
    "    def __init__(self,feature_dim,seq_dim):\n",
    "        super(GraphConvNet, self).__init__()\n",
    "        self.graphlayer1 = GraphConvLayer(feature_dim,feature_dim,seq_dim)\n",
    "        self.graphlayer2 = GraphConvLayer(feature_dim,feature_dim,seq_dim)        \n",
    "        self.fc = nn.Linear(50,3)\n",
    "        \n",
    "    def forward(self,X,A_hat):\n",
    "        A_hat = torch.tensor(A_hat).float()\n",
    "        X1 = self.graphlayer1(X,A_hat)\n",
    "        X2 = self.graphlayer2(X1,A_hat)\n",
    "        \n",
    "        out = self.fc(X2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom GCN with Fixed Adj Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnModel = GraphConvNet(50,31).to(device)\n",
    "gcnModel.fc.load_state_dict(model.fc.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dim = 30\n",
    "\n",
    "optimizer = optim.Adam(gcnModel.parameters(), lr=0.02)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in tqdm_notebook(range(5)):\n",
    "    train_losses = []\n",
    "    for i, (text,label,lengths) in tqdm_notebook(enumerate(tl),total=len(tl)):\n",
    "        textencs = model.encode(text.reshape(seq_dim,-1,1).to(device))\n",
    "        sentenc = model.encode(text.reshape(-1, seq_dim, 1).to(device))\n",
    "        embeds = torch.cat([textencs,sentenc])\n",
    "        label = Variable(label).to(device)\n",
    "        \n",
    "        adj_matrix = torch.tensor(computeAdjMatrix(text)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = gcnModel(embeds,adj_matrix)\n",
    "        \n",
    "        loss = criterion(outputs[-1].reshape(1,-1), label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "#         print(loss.item())\n",
    "    print(np.average(train_losses))\n",
    "    \n",
    "    allLabels = []\n",
    "    allPreds = []\n",
    "    probPreds = []\n",
    "\n",
    "    for i, (text,label,lengths) in enumerate(vl):\n",
    "        labels=[]\n",
    "        textencs = model.encode(text.reshape(seq_dim,-1,1).to(device))\n",
    "        sentenc = model.encode(text.reshape(-1, seq_dim, 1).to(device))\n",
    "        embeds = torch.cat([textencs,sentenc])\n",
    "        label = Variable(label).to(device)\n",
    "\n",
    "        outputs = gcnModel(embeds,adj_matrix)\n",
    "        predicted =  torch.softmax(outputs[-1].reshape(1,-1),1)\n",
    "        predicted = torch.max(predicted, 1)[1].cpu().numpy().tolist()\n",
    "        allLabels += (label.cpu().numpy().tolist())\n",
    "        allPreds += (predicted)\n",
    "\n",
    "    valacc = accuracy_score(allLabels, allPreds)\n",
    "    f1score = f1_score(allLabels, allPreds,average='macro')\n",
    "#         roc = roc_auc_score(allLabels,allPreds)\n",
    "    cr = classification_report(allLabels, allPreds)\n",
    "    print(f'acc: {valacc} f1 {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigf = torch.softmax(gcnModel.graphlayer2.getScores(adj_matrix.float()).cpu().detach(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "plt.matshow(bigf, cmap='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying BSWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dixon_bsws = ['lesbian', 'gay', 'bisexual', 'transgender', 'trans', 'queer', 'lgbt', 'lgbtq', 'homosexual', 'straight', 'heterosexual', 'male', 'female', 'nonbinary',\n",
    "'african', 'african american', 'black', 'white', 'european', 'hispanic', 'latino', 'latina', 'latinx', 'mexican', 'canadian', 'american', 'asian', 'indian',\n",
    "'middle eastern', 'chinese', 'japanese', 'christian', 'muslim', 'jewish', 'buddhist', 'catholic', 'protestant', 'sikh', 'taoist', 'old', 'older', 'young',\n",
    "'younger', 'teenage', 'millenial', 'middle aged', 'elderly', 'blind', 'deaf', 'paralyzed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsws = {}\n",
    "for word in tqdm(word2idx):\n",
    "    vect = torch.tensor([word2idx[word]]).to(device)\n",
    "    textenc = model.encode(vect)\n",
    "    scores = torch.softmax(gcnModel.fc(textenc),1)\n",
    "    \n",
    "    if torch.max(scores) > 0.7 and torch.argmax(scores) == 0:\n",
    "        bsws[word] = torch.max(scores).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelClassification(sentence):\n",
    "    tokens = preprocess(sentence.lower().strip())\n",
    "    output = model(torch.tensor([word2idx[x] for x in tokens]).reshape(1,-1).to(device))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testGraphClassification(sentence):\n",
    "    tokens = preprocess(sentence.lower().strip())\n",
    "    text = torch.tensor([word2idx[x] for x in tokens]).reshape(1,-1).to(device)\n",
    "    \n",
    "    textencs = model.encode(text.reshape(len(tokens),-1,1).to(device))\n",
    "    sentenc = model.encode(text.reshape(-1, len(tokens), 1).to(device))\n",
    "    embeds = torch.cat([textencs,sentenc])\n",
    "\n",
    "    adj_matrix = torch.tensor(computeAdjMatrix(text)).to(device)\n",
    "    \n",
    "    outputs = gcnModel(embeds,adj_matrix)\n",
    "    \n",
    "    return outputs, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOAC(listOfTokenizedSentences, listOfLabels):\n",
    "    tfs = defaultdict(lambda:0)\n",
    "    dfs = defaultdict(lambda:0)\n",
    "    df_pos = defaultdict(lambda:0)\n",
    "    df_neg = defaultdict(lambda:0)\n",
    "    \n",
    "    for i in range(len(listOfTokenizedSentences)):\n",
    "        sent = listOfTokenizedSentences[i]\n",
    "        wordCounts = Counter(sent)\n",
    "        \n",
    "        for word in wordCounts:\n",
    "            tfs[word] += wordCounts[word]\n",
    "            dfs[word] += 1\n",
    "            \n",
    "            if listOfLabels[i] == 0:\n",
    "                df_pos[word] += 1\n",
    "            if listOfLabels[i] == 2:\n",
    "                df_neg[word] += 1\n",
    "                \n",
    "    return tfs,dfs,df_pos,df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfs,dfs,df_pos,df_neg = SOAC(list(train['clean_text']),list(train['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSOAC_BSWs(tfs,dfs,df_pos,df_neg,threshold):\n",
    "    bsws = []\n",
    "    for key in list(tfs.keys()):\n",
    "        if tfs[key] > threshold and df_pos[key] > df_neg[key]:\n",
    "            bsws.append(key)\n",
    "    return bsws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bsws = getSOAC_BSWs(tfs,dfs,df_pos,df_neg,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testModelClassification('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModelClassification('kat is a woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out,adj = testGraphClassification('kat is a woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModelClassification('alice is a woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(torch.tensor([word2idx[x] for x in ['can','you','throw','that','garbage','please']]).reshape(1,-1).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinned Bias Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinned_bias(listOfProbabilities, threshold_type, num_classes=3):\n",
    "    prob_hateful = listOfProbabilities\n",
    "#     print(listOfProbabilities)\n",
    "    \n",
    "    if threshold_type == 'mean':\n",
    "        pb = np.absolute(prob_hateful).sum() / len(listOfProbabilities)\n",
    "        \n",
    "    if threshold_type == 'sym':\n",
    "        num = np.array(prob_hateful) - 1/num_classes\n",
    "        pb = np.absolute(num).sum() / len(listOfProbabilities)\n",
    "        \n",
    "    if threshold_type == 'asym':\n",
    "        num = np.array(prob_hateful) - np.array([min(x,0.5) for x in prob_hateful])\n",
    "        pb = np.absolute(num).sum() / len(listOfProbabilities)\n",
    "    \n",
    "    return pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatefulProbsOfBSWs = [testGraphClassification(word)[0][0][0].item() for word in bsws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pinned_bias(hatefulProbsOfBSWs,'mean'))\n",
    "print(pinned_bias(hatefulProbsOfBSWs,'sym'))\n",
    "print(pinned_bias(hatefulProbsOfBSWs,'asym'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatefulProbsOfBSWs = [testModelClassification(word)[0][0].item() for word in bsws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pinned_bias(hatefulProbsOfBSWs,'mean'))\n",
    "print(pinned_bias(hatefulProbsOfBSWs,'sym'))\n",
    "print(pinned_bias(hatefulProbsOfBSWs,'asym'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
